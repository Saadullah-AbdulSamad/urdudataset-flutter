{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Urdu Writing Dataset Compiler\n",
        "\n",
        "This notebook extracts and compiles the Urdu writing dataset collected through our Android/Flutter application. The dataset contains handwritten Urdu text samples including mono-letter, 2-letter, and 3-letter words.\n",
        "\n",
        "### Overview\n",
        "- **Source**: Firebase Firestore database\n",
        "- **Data Type**: Handwritten Urdu text samples\n",
        "- **Output**: Individual JSON files for each word, packaged as a ZIP file\n",
        "- **Word Types**: Single letters, 2-letter words, and 3-letter words\n",
        "\n",
        "### Prerequisites\n",
        "- Firebase service account key (JSON file)\n",
        "  - A read-only key file **`urdu-dataset-access-key.json`** is available in this repository for public access\n",
        "- Access to the Firestore database containing the dataset\n",
        "\n",
        "### Python Requirements\n",
        "For **Google Colab** (recommended):\n",
        "- `firebase-admin` (install via: `!pip install firebase-admin`)\n",
        "\n",
        "For **local environments**:\n",
        "```bash\n",
        "pip install firebase-admin\n",
        "```\n",
        "\n",
        "**Optional for local development:**\n",
        "```bash\n",
        "pip install jupyter notebook  # If using Jupyter locally\n",
        "```\n",
        "\n",
        "### Environment Compatibility\n",
        "- **Designed for**: Google Colab environment\n",
        "- **Local/Different environments**: Some modifications may be needed for local Jupyter or other environments:\n",
        "  - Replace `files.upload()` and `files.download()` with appropriate file handling\n",
        "  - Install required packages: `pip install firebase-admin`\n",
        "  - Adjust file paths as needed for your local setup\n",
        "\n",
        "### ‚ö†Ô∏è Technical Note: Urdu Text Handling\n",
        "Since this dataset contains **Urdu text**, you may encounter encoding or display issues in:\n",
        "- **Python console output** (depending on terminal/OS support for RTL scripts)\n",
        "- **JSON viewers/IDEs** (may not render Urdu characters properly)\n",
        "- **File systems** (filename encoding issues on some OS)\n",
        "\n",
        "**Recommendations for developers:**\n",
        "- Use UTF-8 compatible editors and terminals\n",
        "- Ensure your system has Urdu font support installed\n",
        "- JSON data integrity remains intact regardless of display issues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Firebase Authentication Setup\n",
        "\n",
        "Upload your Firebase service account key file and initialize the Firebase Admin SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxQQbC40huUz"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials\n",
        "from google.colab import files\n",
        "\n",
        "# Upload the Firebase service account key file\n",
        "print(\"Please upload your Firebase service account key (urdu-dataset-access-key.json):\")\n",
        "uploaded = files.upload()\n",
        "uploaded_file = next(iter(uploaded))\n",
        "\n",
        "# Initialize Firebase Admin SDK\n",
        "cred = credentials.Certificate(uploaded_file)\n",
        "app = firebase_admin.initialize_app(cred)\n",
        "print(\"‚úÖ Firebase initialized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjdbGDrkill0"
      },
      "outputs": [],
      "source": [
        "# Connect to Firestore database\n",
        "from firebase_admin import firestore\n",
        "db = firestore.client(app=app)\n",
        "\n",
        "# Access the dataset collection\n",
        "collection_ref = db.collection(\"dataset\")\n",
        "print(\"‚úÖ Connected to Firestore database\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Data Retrieval\n",
        "\n",
        "Fetch all dataset documents from Firestore with version 2 (latest version)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Database Connection\n",
        "\n",
        "Connect to the Firestore database and access the dataset collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CIWq90HiqRR"
      },
      "outputs": [],
      "source": [
        "# Query documents with version 2 (latest dataset version)\n",
        "from firebase_admin.firestore import FieldFilter\n",
        "\n",
        "docs = collection_ref.where(filter=FieldFilter(\"version\", '==', 2)).stream()\n",
        "all_documents = [doc.to_dict() for doc in docs]\n",
        "\n",
        "print(f\"üìä Retrieved {len(all_documents)} documents from the dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Dataset Analysis\n",
        "\n",
        "Analyze the collected data to understand the dataset composition and contributor statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-meUPn36i9i"
      },
      "outputs": [],
      "source": [
        "# Analyze unique contributors\n",
        "user_ids = set()\n",
        "for doc in all_documents:\n",
        "    if 'userID' in doc:\n",
        "        user_ids.add(doc['userID'])\n",
        "\n",
        "print(f\"üë• Number of unique contributors: {len(user_ids)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YI_sEpdbMtj"
      },
      "outputs": [],
      "source": [
        "# Analyze contribution sessions (first 4 characters of user ID represent session)\n",
        "from collections import Counter\n",
        "\n",
        "first_four_chars = [user_id[:4] for user_id in user_ids]\n",
        "session_counts = Counter(first_four_chars)\n",
        "\n",
        "print(\"üìà Contributors per session:\")\n",
        "for session, count in sorted(session_counts.items()):\n",
        "    print(f\"  Session {session}: {count} contributors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpDf8zQRk7aD"
      },
      "outputs": [],
      "source": [
        "# Display sample document structure\n",
        "print(f\"üìã Sample document structure:\")\n",
        "print(f\"Total documents: {len(all_documents)}\")\n",
        "if all_documents:\n",
        "    sample_doc = all_documents[0]\n",
        "    print(f\"Sample document keys: {list(sample_doc.keys())}\")\n",
        "    if 'data' in sample_doc:\n",
        "        print(f\"Words in sample document: {list(sample_doc['data'].keys())[:5]}...\")  # Show first 5 words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Data Processing and Grouping\n",
        "\n",
        "Process the raw data and group handwriting samples by word for easier analysis and usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgFuZPV_UfJi"
      },
      "outputs": [],
      "source": [
        "# Group handwriting samples by word\n",
        "from collections import defaultdict\n",
        "\n",
        "grouped_data = defaultdict(list)\n",
        "total_samples = 0\n",
        "\n",
        "# Process each document and extract handwriting data\n",
        "for doc in all_documents:\n",
        "    if 'data' in doc:\n",
        "        data = doc['data']\n",
        "        for word in data:\n",
        "            # Each word contains handwriting sample data\n",
        "            grouped_data[word].append(data[word])\n",
        "            total_samples += 1\n",
        "\n",
        "print(f\"üî§ Processed {total_samples} handwriting samples\")\n",
        "print(f\"üìù Number of unique words: {len(grouped_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9Rftt21DX6V8"
      },
      "outputs": [],
      "source": [
        "# Display words categorized by length\n",
        "mono_letters = []\n",
        "two_letters = []\n",
        "three_letters = []\n",
        "\n",
        "for word in grouped_data:\n",
        "    if len(word) == 1:\n",
        "        mono_letters.append(word)\n",
        "    elif len(word) == 2:\n",
        "        two_letters.append(word)\n",
        "    elif len(word) == 3:\n",
        "        three_letters.append(word)\n",
        "\n",
        "print(f\"üìä Dataset composition:\")\n",
        "print(f\"  ‚Ä¢ Single letters: {len(mono_letters)} words\")\n",
        "print(f\"  ‚Ä¢ Two-letter words: {len(two_letters)} words\") \n",
        "print(f\"  ‚Ä¢ Three-letter words: {len(three_letters)} words\")\n",
        "\n",
        "print(f\"\\nüî§ Sample words:\")\n",
        "print(f\"  ‚Ä¢ Single letters: {mono_letters[:10]}\")\n",
        "print(f\"  ‚Ä¢ Two-letter: {two_letters[:10]}\")\n",
        "print(f\"  ‚Ä¢ Three-letter: {three_letters[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD141PZE0MPN"
      },
      "outputs": [],
      "source": [
        "# Display sample count for a specific word (example: ÿπÿ∑ÿ±)\n",
        "example_word = 'ÿπÿ∑ÿ±'\n",
        "if example_word in grouped_data:\n",
        "    print(f\"‚úèÔ∏è Number of handwriting samples for '{example_word}': {len(grouped_data[example_word])}\")\n",
        "else:\n",
        "    # Find the first available word as an example\n",
        "    example_word = list(grouped_data.keys())[0]\n",
        "    print(f\"‚úèÔ∏è Number of handwriting samples for '{example_word}': {len(grouped_data[example_word])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Export to JSON Files\n",
        "\n",
        "Create individual JSON files for each word containing all handwriting samples, then package them into a ZIP file (with JSONs directly, not in a subdirectory) for download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCbBXr1ol_qS"
      },
      "outputs": [],
      "source": [
        "# Create JSON files for each word\n",
        "import json\n",
        "import os\n",
        "\n",
        "print(\"üìÅ Creating JSON files in current directory...\")\n",
        "\n",
        "# Export each word's data to a separate JSON file\n",
        "exported_count = 0\n",
        "for word in grouped_data:\n",
        "    try:\n",
        "        filename = f'{word}.json'\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(grouped_data[word], f, indent=2, ensure_ascii=False)\n",
        "        exported_count += 1\n",
        "        \n",
        "        # Show progress for every 10 files\n",
        "        if exported_count % 10 == 0:\n",
        "            print(f\"  ‚úÖ Exported {exported_count}/{len(grouped_data)} files...\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error exporting '{word}': {str(e)}\")\n",
        "\n",
        "print(f\"üéâ Successfully exported {exported_count} JSON files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kjZcV5frZg7p"
      },
      "outputs": [],
      "source": [
        "# Create ZIP archive of all JSON files\n",
        "print(\"üì¶ Creating ZIP archive...\")\n",
        "!zip -r dataset.zip *.json\n",
        "print(\"‚úÖ ZIP archive 'dataset.zip' created successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJDOvKDAZi31"
      },
      "outputs": [],
      "source": [
        "# Download the ZIP file\n",
        "print(\"‚¨áÔ∏è Initiating download...\")\n",
        "files.download('dataset.zip')\n",
        "print(\"üéä Dataset compilation complete! Your ZIP file has been downloaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary\n",
        "\n",
        "This notebook successfully compiled the Urdu handwriting dataset from Firestore into individual JSON files. Each JSON file contains all handwriting samples for a specific word, making it easy to:\n",
        "\n",
        "- **Train machine learning models** for Urdu handwriting recognition\n",
        "- **Analyze handwriting patterns** across different contributors\n",
        "- **Study character formation** in Urdu script\n",
        "- **Research linguistic patterns** in mono-letter, 2-letter, and 3-letter combinations\n",
        "\n",
        "#### Output Structure\n",
        "- **File format**: One JSON file per word\n",
        "- **Naming convention**: `{word}.json`\n",
        "- **Content**: Array of handwriting sample data for each word\n",
        "- **Encoding**: UTF-8 to properly handle Urdu text\n",
        "\n",
        "#### Dataset Information\n",
        "- **Source**: Flutter mobile application for Urdu handwriting collection\n",
        "- **Contributors**: Multiple users across different data collection sessions\n",
        "- **Word types**: Single letters, 2-letter words, and 3-letter words\n",
        "- **Data version**: Version 2 (latest)\n",
        "\n",
        "The compiled dataset is now ready for research and development purposes!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
